{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agaier/OpenELM/blob/main/lmx_sentiment_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LMX Sentiment Demo\n",
        "\n",
        "Implements simple quality diversity evolutionary algorithm (MAP-Elites) that searches the trade-off between similarity (as measured by an embedding model) and changed sentiment of an input sentence (e.g. it searches for how to maximally change sentiment while minimally modifying the sentence). Variation is generated by language model crossover (https://arxiv.org/abs/2302.12170). "
      ],
      "metadata": {
        "id": "3xcrxujEfL-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzGV8iYoQk3Q",
        "outputId": "722bbd04-1d96-428b-a8ac-b6207cc2293a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=884201fe1424fd5cc27ce33724be3031c4c9a2507e4f8d37e102d5d6f15ff11f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "#can sub in whatever language model you want here...\n",
        "#model_name = \"EleutherAI/pythia-350m\"\n",
        "model_name = \"EleutherAI/pythia-1.4b-deduped\"\n",
        "\n",
        "#device=0 = GPU; device=-1 = CPU\n",
        "generator = pipeline('text-generation', model=model_name,device=0)  # for generating text\n",
        "embed = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',device=0) # for measuring diversity\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",device=0) # for measuring quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDYzfaCWL9gg"
      },
      "source": [
        "# Evolution code\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW9ArecvMC_3"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "from torch.cuda import seed_all\n",
        "import numpy as np\n",
        "from tensorflow.python.ops import gen_batch_ops\n",
        "\n",
        "generator.tokenizer.pad_token_id = generator.model.config.eos_token_id\n",
        "\n",
        "# euclidean distance\n",
        "def dist(x,y):\n",
        "  return np.sqrt( ((x-y)**2).sum() )\n",
        "\n",
        "#make simple \"crossover\" prompt composed of random genomes from pop\n",
        "def create_crossover_prompt(examples):\n",
        "  prompt = \"\"\n",
        "  for candidate in examples:\n",
        "    prompt+=candidate+\"\\n\"\n",
        "  return prompt\n",
        "\n",
        "#extract candidate offspring from response from LM\n",
        "def sentiment_process_output(output,take_offspring=3,verbose=False):\n",
        "  candidates = []\n",
        "  genomes = output.split(\"\\n\")[:-1]\n",
        "  genomes = [x.strip() for x in genomes]\n",
        "  for genome in genomes[:take_offspring]:\n",
        "      if len(genome)<10:\n",
        "        if verbose: print(genome,'wrong length')\n",
        "        continue\n",
        "      if verbose: print(genome)\n",
        "      candidates.append(genome)\n",
        "  return list(set(candidates))\n",
        "\n",
        "# bootstrap an initial population from using LMX on a few seed sentences\n",
        "def sentiment_bootstrap_population_lmx(size=196,seed_sentences=[],batch_size=64,**kwargs):\n",
        "  pop=seed_sentences[:]\n",
        "  idx = 0\n",
        "  num_examples = len(seed_sentences)\n",
        "\n",
        "  while len(pop)<size:\n",
        "    last_example = seed_sentences[idx%len(seed_sentences)]\n",
        "    all_other_examples = set(seed_sentences)-set([last_example])\n",
        "    #print(last_example,all_other_examples)\n",
        "    examples = list(all_other_examples)\n",
        "    random.shuffle(examples)\n",
        "    examples = examples + [last_example]\n",
        "    \n",
        "    #print(examples)\n",
        "    #print(\"doing crossover...\")\n",
        "    raw_crossovers = do_crossover_fast(None,chosen_examples=examples,examples=num_examples,batch_size=batch_size)\n",
        "    for raw_crossover in raw_crossovers:\n",
        "      pop+= process_output(raw_crossover,take_offspring=1)\n",
        "\n",
        "    idx+=1\n",
        "  return pop[:size]\n",
        "\n",
        "# slight variant (always place target sentence at the end of few-shot prompt)\n",
        "def sentiment_bootstrap_population_lmx_2(size=196,seed_sentences=[],target_sentence=None,batch_size=64,**kwargs):\n",
        "  pop=seed_sentences[:]\n",
        "  num_examples = len(seed_sentences)\n",
        "\n",
        "  while len(pop)<size:\n",
        "    last_example = target_sentence\n",
        "    all_other_examples = set(seed_sentences)-set([last_example])\n",
        "    #print(last_example,all_other_examples)\n",
        "    examples = list(all_other_examples)\n",
        "    random.shuffle(examples)\n",
        "    examples = examples + [last_example]\n",
        "    \n",
        "    #print(examples)\n",
        "    #print(\"doing crossover...\")\n",
        "    raw_crossovers = do_crossover_fast(None,chosen_examples=examples,examples=num_examples,batch_size=batch_size)\n",
        "    for raw_crossover in raw_crossovers:\n",
        "      pop+= process_output(raw_crossover,take_offspring=1)\n",
        "\n",
        "  return pop[:size]\n",
        "\n",
        "\n",
        "# batched LMX crossover\n",
        "def do_crossover_fast(pop,examples=5,temp=1.0,batch_size=2,max_tokens=128,chosen_examples=None,prompt_generator=create_crossover_prompt):\n",
        "  if chosen_examples==None:\n",
        "    chosen_examples = np.random.choice(pop,examples,replace=False)\n",
        "\n",
        "  prompt = prompt_generator(chosen_examples)\n",
        "\n",
        "  model_output = generator([prompt]*batch_size,\n",
        "                         batch_size=batch_size,\n",
        "                         do_sample=True,\n",
        "                         max_length=max_tokens,\n",
        "                         temperature=temp,\n",
        "                         #top_p=0.8,\n",
        "                         #top_k=30,\n",
        "                         #penalty_alpha=0.4, top_k=10,\n",
        "                         return_full_text=False)\n",
        "  output = [x[0]['generated_text'] for x in model_output]\n",
        "  return output\n",
        "\n",
        "# simple 1D map-elites implementation\n",
        "class map_elites1d:\n",
        "  def __init__(self,fitness_func,initial_pop,grid_sz=40,method=\"crossover\",batch_size=8,temperature=1.0):\n",
        "    self.temperature=temperature\n",
        "    self.grid={}\n",
        "    self.fitness_func = fitness_func\n",
        "    self.grid_sz = grid_sz\n",
        "    self.log = []\n",
        "    self.evals = 0\n",
        "    self.initial_pop = initial_pop\n",
        "    self.method = method\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    for indiv in initial_pop:\n",
        "      indiv = indiv.strip()\n",
        "      fit,bc = self.fitness_func(indiv)\n",
        "      self.evals+=1\n",
        "      #print(fit,bc)\n",
        "      self.insert(indiv,fit,bc)\n",
        "\n",
        "  def eval_insert(self,indivs):\n",
        "    for indiv in indivs:\n",
        "      indiv = indiv.strip()\n",
        "      fit,bc = self.fitness_func(indiv)\n",
        "      self.evals+=1\n",
        "      #print(fit,bc)\n",
        "      self.insert(indiv,fit,bc)\n",
        "    self.print_stats()\n",
        "\n",
        "  def export(self):\n",
        "    data = {}\n",
        "    data['grid'] = self.grid\n",
        "    data['log'] = self.log\n",
        "    data['evals'] = self.evals\n",
        "    data['method'] = self.method\n",
        "    return data\n",
        "\n",
        "  def to_bin(self,bc):\n",
        "    bin = int(bc*(self.grid_sz))\n",
        "    bin = min(self.grid_sz-1,bin)\n",
        "    return bin\n",
        "  \n",
        "  def insert(self,indiv,fit,bc):\n",
        "    bin = self.to_bin(bc)\n",
        "    if bin not in self.grid or self.grid[bin]['fitness']<fit:\n",
        "      self.grid[bin]={'fitness':fit,'bc':bc,'genome':indiv}\n",
        "\n",
        "  def do_mutation(self):\n",
        "    full_pop = self.gather_pop()\n",
        "    parent = random.choice(full_pop)\n",
        "    raw_mutations = do_mutation([parent],batch_size=self.batch_size)\n",
        "    candidates = []\n",
        "    for raw_mutation in raw_mutations:\n",
        "      candidates += process_output(raw_mutation,take_offspring=1)\n",
        "    for candidate in candidates:\n",
        "      fit,bc = self.fitness_func(candidate)\n",
        "      self.evals+=1\n",
        "      self.insert(candidate,fit,bc)\n",
        "\n",
        "  # LMX biased to use individuals nearby in map (sometimes increases performance)\n",
        "  def do_crossover_near(self):\n",
        "    full_pop = self.gather_pop()\n",
        "\n",
        "    num_examples = 3\n",
        "\n",
        "    if len(full_pop)<num_examples:\n",
        "      print(\"...adding examples from initial pop...\")\n",
        "    while len(full_pop)<num_examples:\n",
        "      full_pop.append(random.choice(self.initial_pop))\n",
        "\n",
        "    idx = random.randint(0,len(full_pop)-1)\n",
        "    dists = np.array([1.0 / (1+abs(idx-x)**3) for x in range(len(full_pop))])\n",
        "    dists /= np.sum(dists)\n",
        "    examples = np.random.choice(full_pop,p=dists,replace=False,size=num_examples)\n",
        "    examples = list(examples)\n",
        "\n",
        "    raw_crossovers = do_crossover(None,chosen_examples=examples,examples=num_examples,batch_size=self.batch_size,temp=self.temperature)\n",
        "\n",
        "    candidates = []\n",
        "    for raw_crossover in raw_crossovers:\n",
        "      candidates += process_output(raw_crossover,take_offspring=1)\n",
        "\n",
        "    candidates = [candidate.strip() for candidate in candidates]\n",
        "\n",
        "    for candidate in candidates:\n",
        "      fit,bc = self.fitness_func(candidate)\n",
        "      self.evals+=1\n",
        "      self.insert(candidate,fit,bc)\n",
        "  \n",
        "  \n",
        "  # LMX with random parents from map\n",
        "  def do_crossover(self):\n",
        "    full_pop = self.gather_pop()\n",
        "\n",
        "    num_examples = 3\n",
        "\n",
        "    if len(full_pop)<num_examples:\n",
        "      print(\"...adding examples from initial pop...\")\n",
        "    while len(full_pop)<num_examples:\n",
        "      full_pop.append(random.choice(self.initial_pop))\n",
        "\n",
        "    raw_crossovers = do_crossover(full_pop,examples=num_examples,batch_size=self.batch_size,temp=self.temperature)\n",
        "\n",
        "    candidates = []\n",
        "    for raw_crossover in raw_crossovers:\n",
        "      candidates += process_output(raw_crossover,take_offspring=1)\n",
        "\n",
        "    candidates = [candidate.strip() for candidate in candidates]\n",
        "    for candidate in candidates:\n",
        "      fit,bc = self.fitness_func(candidate)\n",
        "      self.evals+=1\n",
        "      self.insert(candidate,fit,bc)\n",
        "      \n",
        "  # main evolution loop\n",
        "  def do_evals(self,num_evals=300):\n",
        "    idx=0\n",
        "    while self.evals < num_evals:\n",
        "      if self.method=='crossover':\n",
        "        self.do_crossover()\n",
        "      elif self.method=='crossover-near':\n",
        "        self.do_crossover_near()\n",
        "      elif self.method=='crossover-mixed':\n",
        "        if random.random() < 0.5:\n",
        "          self.do_crossover()\n",
        "        else:\n",
        "          self.do_crossover_near()\n",
        "      else:\n",
        "        raise \"no variation operator by name {name}\".format(name=self.method)\n",
        "      if idx%1==0:\n",
        "        print(idx,self.evals)\n",
        "        self.print_stats()\n",
        "      idx+=1\n",
        "\n",
        "  def gather_pop(self):\n",
        "    keys = list(self.grid.keys())\n",
        "    keys.sort()\n",
        "    pop = [self.grid[key]['genome'] for key in keys]\n",
        "    return pop\n",
        "\n",
        "  def print_stats(self):\n",
        "    niches = len(list(self.grid.keys()))\n",
        "    qd_score = sum([self.grid[key]['fitness'] for key in self.grid.keys()])\n",
        "    pop = self.gather_pop()\n",
        "    pts = [(self.grid[key]['fitness'],self.grid[key]['bc']) for key in self.grid.keys()]\n",
        "    self.log.append({'qd_score':qd_score,'niches':niches,'pts':pts,'evals':self.evals})\n",
        "    print('# niches: {niches}, qd score: {qd_score}'.format(niches=niches,qd_score=qd_score))\n",
        "\n",
        "def mapelites_sentiment_fitness(x):\n",
        "  global root_embedding\n",
        "  global sentiment_class\n",
        "  res = sentiment_analysis(x,top_k=3)\n",
        "  emb = embed.encode([x])[0]\n",
        "  d = dist(emb,root_embedding)\n",
        "  bc = d\n",
        "  for k in res:\n",
        "    if k['label']==sentiment_class:\n",
        "      #print(x,k['score'])\n",
        "      fit = k['score']\n",
        "      break\n",
        "  #normalize bc between 0 and 1.25\n",
        "  bc = max(0,bc)\n",
        "  bc = min(1.25,bc)\n",
        "  bc = bc/1.25\n",
        "  return fit,bc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irnidA1vWr9A"
      },
      "outputs": [],
      "source": [
        "# some default sentence collections \n",
        "negative_sentences = [\"The moon is a boring sky rock.\",\"Puppies are smelly, ill-behaved animals that make everything worse.\",\"I really really hated my trip to the amusement park, I threw up on a roller coaster and was robbed by a Elvis impersonator.\"]\n",
        "negative_quotes = [\"Whenever a friend succeeds, a little something in me dies.\",\"Kids, you tried your best and you failed miserably. The lesson is, never try.\",\"Life is divided into the horrible and the miserable.\"]\n",
        "#positive_sentences = [\"Chocolate is such a very, very amazing dessert.\",\"I really love my pet iguana, Iggy.\",\"Rainbows and waterfalls are so beautiful -- they make me so very happy.\"]\n",
        "#positive_quotes = ['Few things can help an individual more than to place responsibility on him, and to let him know that you trust him.','Be the change that you wish to see in the world.','When the sun is shining I can do anything; no mountain is too high, no trouble too difficult to overcome.']\n",
        "\n",
        "create_population = sentiment_bootstrap_population_lmx\n",
        "do_crossover = do_crossover_fast\n",
        "process_output = sentiment_process_output\n",
        "\n",
        "root_sentence = negative_sentences[0]\n",
        "root_embedding = None\n",
        "\n",
        "# sentiment class to target -- recommend \"positive\" or \"neutral\"; \"negative\" often generates toxic output\n",
        "sentiment_class = \"positive\"\n",
        "seed_sentences = negative_sentences\n",
        "\n",
        "def do_run(_root_sentence,_sentiment_class,total_evals=1200,method=\"crossover\",initial_pop=None,batch_size=64):\n",
        "  global sentiment_class,root_sentence,root_embedding\n",
        "  root_sentence = _root_sentence\n",
        "  sentiment_class = _sentiment_class\n",
        "  root_embedding = embed.encode([root_sentence])[0]\n",
        "\n",
        "  pop=initial_pop\n",
        "\n",
        "  me = map_elites1d(mapelites_sentiment_fitness,pop,grid_sz=30,method=method,batch_size=batch_size,temperature=0.8)\n",
        "  me.do_evals(total_evals)\n",
        "  return me"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evolution"
      ],
      "metadata": {
        "id": "-FJdvt-_hFcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVeLxsrMtWXW"
      },
      "outputs": [],
      "source": [
        "#decrease batch size if you run into memory issues\n",
        "batch_size = 64\n",
        "\n",
        "# one of the examples from the paper; note that \"seeds\" should be of length ~3 and contain one copy of the target sentence\n",
        "target_sentence = negative_quotes[1]\n",
        "sentiment = \"positive\"\n",
        "seeds = negative_quotes\n",
        "\n",
        "# simple custom example\n",
        "#target_sentence = \"The terrible day would never end.\"\n",
        "#sentiment = \"positive\"\n",
        "#seeds = [target_sentence, \"The awful day would never end.\",\"The bad day seemed like it would never end.\"]\n",
        "\n",
        "print(\"Target sentence:\", target_sentence)\n",
        "print(\"Seeds:\",seeds)\n",
        "\n",
        "initial_pop = create_population(size=batch_size * 5,seed_sentences=seeds,target_sentence=target_sentence,batch_size=batch_size)\n",
        "evals = 2500\n",
        "results = do_run(target_sentence,sentiment,evals,\"crossover-near\",initial_pop=initial_pop,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display results"
      ],
      "metadata": {
        "id": "IkKbOkaahVBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pylab\n",
        "me = results\n",
        "\n",
        "print(me.log[-1]['qd_score'])\n",
        "keys = list(me.grid.keys())\n",
        "fit = [me.grid[key]['fitness'] for key in keys]\n",
        "bc = [me.grid[key]['bc'] for key in keys]\n",
        "genome = [me.grid[key]['genome'] for key in keys]\n",
        "\n",
        "def dominated(b,a):\n",
        "  return a[0]<b[0] and a[1]>b[1]\n",
        "\n",
        "is_dominated = [False]*len(bc)\n",
        "for i in range(len(bc)):\n",
        "    for j in range(len(bc)):\n",
        "        if dominated((bc[i],fit[i]),(bc[j],fit[j])):\n",
        "            is_dominated[i] = True\n",
        "            break\n",
        "\n",
        "# remove dominated points\n",
        "bc = [bc[i] for i in range(len(bc)) if not is_dominated[i]]\n",
        "fit = [fit[i] for i in range(len(fit)) if not is_dominated[i]]\n",
        "genome = [genome[i] for i in range(len(genome)) if not is_dominated[i]]\n",
        "\n",
        "#plot pareto front\n",
        "pylab.figure(figsize=(10,3.5))\n",
        "# make font bigger\n",
        "pylab.rcParams.update({'font.size': 19})\n",
        "# make points bigger\n",
        "pylab.rcParams['lines.markersize'] = 10\n",
        "\n",
        "pylab.plot(bc,fit,\"ro\")\n",
        "pylab.ylabel(\"Sentiment Score\")\n",
        "pylab.xlabel(\"Distance from Seed\")\n",
        "\n",
        "pylab.show()\n",
        "res = list(zip(bc,fit,genome))\n",
        "res.sort(key=lambda x: x[0])\n",
        "\n",
        "#print out full results\n",
        "print(\"Distance | Sentiment | Genome\")\n",
        "for ind in res:\n",
        "    _bc,_fit,_genome = ind\n",
        "    # format bc and fit to be 2 decimal places\n",
        "    _bc = \"{:.2f}\".format(_bc)\n",
        "    _fit = \"{:.2f}\".format(_fit)\n",
        "    # write to a latex table\n",
        "    print(f\"{_bc} & {_fit} & {_genome} \")"
      ],
      "metadata": {
        "id": "o_Ff7T5FcVNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}